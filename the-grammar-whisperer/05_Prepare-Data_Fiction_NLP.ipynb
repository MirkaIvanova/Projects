{"cells":[{"cell_type":"markdown","metadata":{"id":"FVf5htUiok_w"},"source":["v0.1"]},{"cell_type":"markdown","metadata":{"id":"phCTV-F1_-Iw"},"source":["> TODO:\n","- input:\n","  - f\"{data_clean_dir}/bg_fiction_all.tsv\"\n","- output:\n","  - f'{data_processed_dir}/sent_fiction_nlp_features_part1_v1_checkpoint.tsv'\n","  - f'{data_processed_dir}/sent_fiction_nlp_features_part1_v1.tsv'\n","  - f'{data_processed_dir}/sent_fiction_nlp_features_part1_v2.tsv'\n","  - f'{data_processed_dir}/sent_fiction_nlp_features_part1_v3_final.tsv'    \n","- df.copy(deep=True)\n","- remove the warning, start_time\n","- add descriptions + fix headings\n","- remove yellow\n","- extract all functions to py files\n","- run locally\n","- check typos"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3286,"status":"ok","timestamp":1737994251209,"user":{"displayName":"Miroslava Ivanova","userId":"15457167696903657486"},"user_tz":-120},"id":"vxnqkrnjqPEK","outputId":"ba1e2921-c8b3-4142-df45-bccf5516ed43"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: ufal.udpipe in /usr/local/lib/python3.11/dist-packages (1.3.1.1)\n"]}],"source":["!pip install ufal.udpipe\n","# !pip install stanza\n","# !pip install spacy-stanza"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"kIBV3jB0eyjZ","executionInfo":{"status":"ok","timestamp":1737994252175,"user_tz":-120,"elapsed":969,"user":{"displayName":"Miroslava Ivanova","userId":"15457167696903657486"}}},"outputs":[],"source":["import ast\n","import os\n","import pandas as pd\n","import polars as pl\n","import sys\n","import time\n","import urllib.request\n","\n","from ufal.udpipe import Model, Pipeline, ProcessingError"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":280,"status":"ok","timestamp":1737994252452,"user":{"displayName":"Miroslava Ivanova","userId":"15457167696903657486"},"user_tz":-120},"id":"WyumQPc64fls"},"outputs":[],"source":["!if [ ! -f \"/content/helpers/save_load_checkpoint_files.py\" ]; then wget -P helpers/ https://raw.githubusercontent.com/MirkaIvanova/Projects/refs/heads/main/the-grammar-whisperer/helpers/save_load_checkpoint_files.py; fi\n","!if [ ! -f \"/content/helpers/parse_nlp_features.py\" ]; then wget -P helpers/ https://raw.githubusercontent.com/MirkaIvanova/Projects/refs/heads/main/the-grammar-whisperer/helpers/parse_nlp_features.py; fi\n","!if [ ! -f \"/content/helpers/parse_nlp_morphtags.py\" ]; then wget -P helpers/ https://raw.githubusercontent.com/MirkaIvanova/Projects/refs/heads/main/the-grammar-whisperer/helpers/parse_nlp_morphtags.py; fi\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"c5Ih6ZiUcYIW","executionInfo":{"status":"ok","timestamp":1737994252452,"user_tz":-120,"elapsed":4,"user":{"displayName":"Miroslava Ivanova","userId":"15457167696903657486"}}},"outputs":[],"source":["IS_GUEST = False\n","LOAD_SAVED_DATA = True"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4500,"status":"ok","timestamp":1737994256948,"user":{"displayName":"Miroslava Ivanova","userId":"15457167696903657486"},"user_tz":-120},"id":"LAbKciIFaZx5","outputId":"88d14dcd-30ee-4b7e-ae75-23e401151581"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["if IS_GUEST:\n","    root_dir = '.'\n","else:\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount=True)\n","    root_dir = \"/content/drive/MyDrive/softuni/the-grammar-whisperer\"\n","\n","data_clean_dir = f\"{root_dir}/data/clean\"\n","data_processed_dir = f\"{root_dir}/data/processed\"\n","helpers_dir = f\"{root_dir}/helpers\"\n","\n","if root_dir not in sys.path:\n","    sys.path.append(root_dir)\n","    sys.path.append(helpers_dir)\n","\n","bg_fiction_clean_csv =  f\"{data_clean_dir}/bg_fiction_all.tsv\""]},{"cell_type":"code","execution_count":7,"metadata":{"id":"oAiGM2Kz4aOg","executionInfo":{"status":"ok","timestamp":1737994259188,"user_tz":-120,"elapsed":2243,"user":{"displayName":"Miroslava Ivanova","userId":"15457167696903657486"}}},"outputs":[],"source":["from save_load_checkpoint_files import save_checkpoint, load_checkpoint\n","from parse_nlp_morphtags import parse_nlp_morphtags\n","from parse_nlp_features import process_chunk"]},{"cell_type":"markdown","metadata":{"id":"KdiRVap4fEBf"},"source":["# Add lemmas and pos tagging to fiction sentences"]},{"cell_type":"markdown","metadata":{"id":"HxqgGBudioW9"},"source":["#### Load clean csv with sentences"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19192,"status":"ok","timestamp":1737993582581,"user":{"displayName":"Miroslava Ivanova","userId":"15457167696903657486"},"user_tz":-120},"id":"yYnPiJ63WfE9","outputId":"1b4c89c6-1b3d-4f70-b6d8-092eb7109778"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(6690845, 1)"]},"metadata":{},"execution_count":6}],"source":["df_polars = pl.read_csv(bg_fiction_clean_csv, separator='\\t')\n","df = df_polars.to_pandas()\n","del df_polars\n","\n","df = df.rename(columns={\"text\": \"sentence\"})\n","df.shape"]},{"cell_type":"markdown","metadata":{"id":"wTOlydwLD0mL"},"source":["#### Initialize UDPipe and download Bulgarian model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y0m4xyo9lBkF"},"outputs":[],"source":["# Download the UDPipe Bulgarian model\n","model_path = \"bulgarian-btb-ud-2.5-191206.udpipe\"\n","\n","if not os.path.exists(model_path):\n","    model_url_udpipe = \"https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-3131/bulgarian-btb-ud-2.5-191206.udpipe?sequence=6&isAllowed=y\"\n","    urllib.request.urlretrieve(model_url_udpipe, model_path)\n","\n","model = Model.load(model_path)\n","pipeline = Pipeline(model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')"]},{"cell_type":"markdown","metadata":{"id":"2wydS4HBANtb"},"source":["###### Function to process one sentence into pos, features, etc."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9FshOPLYs__w"},"outputs":[],"source":["# ðŸ’™mii move to py\n","def convert_udpipe_to_spacy(text):\n","    doc_str = pipeline.process(text)\n","\n","    lines = doc_str.strip().splitlines()\n","    words = []\n","    lemmas = []\n","    spaces = []\n","    pos_tags = []\n","    morph_tags = []\n","    features = []\n","    dep_rels = []\n","\n","    for line in lines:\n","        if line.startswith(\"#\") or not line.strip():\n","            continue\n","\n","        parts = line.split(\"\\t\")\n","        index, word, lemma, pos, tag, feats, head, dep_rel, _, misc = parts\n","\n","        words.append(word)\n","        lemmas.append(lemma)\n","        pos_tags.append(pos)  # POS tag (simpler POS category)\n","        morph_tags.append(tag)  # Detailed morphological tag\n","        features.append(feats)\n","        dep_rels.append(dep_rel)\n","\n","        if \"SpaceAfter=No\" in misc:\n","            spaces.append(False)\n","        else:\n","            spaces.append(True)\n","\n","    return words, lemmas, pos_tags, morph_tags, features, dep_rels, len(words)"]},{"cell_type":"markdown","metadata":{"id":"fEhXJKXsmL_m"},"source":["###### Process the entire clean dataframe in chunks using checkpoints"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M9J0X7rxAln7"},"outputs":[],"source":["# ðŸ’™mii move to .py file\n","def find_unprocessed_indices(df, col):\n","    \"\"\"Finds the indices of rows in the DataFrame where the column is NaN, assuming those rows are unprocessed.\"\"\"\n","\n","    # Check if the column exists\n","    if col not in df.columns:\n","        return df.index.tolist()\n","\n","    # Find unprocessed rows where the value in the column is NaN\n","    unprocessed_mask = df[col].isna()\n","    unprocessed_indices = df[unprocessed_mask].index.tolist()\n","\n","    return unprocessed_indices\n","\n","def process_dataframe_with_checkpoints(_df, process_func, chunk_size=1000, output_path='final_output.csv', sep='.'):\n","    \"\"\"\n","    Process a DataFrame in chunks with checkpoint capability using vectorized operations.\n","\n","    Args:\n","        _df: Input DataFrame\n","        process_func: Function to apply to each chunk\n","        chunk_size: Number of rows to process in each chunk\n","        checkpoint_path: Path to save checkpoint files\n","        output_path: Path to save final output\n","    \"\"\"\n","\n","    df, checkpoint_filename = load_checkpoint(output_path, sep=sep)\n","    if df is None:\n","        df = _df.copy()\n","        print(\"Starting fresh processing...\")\n","    else:\n","        print(f\"Loaded checkpoint {checkpoint_filename} with {len(df)} rows\")\n","\n","    if False:\n","        if os.path.exists(checkpoint_path):\n","            df = pd.read_csv(checkpoint_path, sep=sep)\n","            print(f\"Loaded checkpoint with {len(df)} rows\")\n","        else:\n","            df = _df.copy()\n","            print(\"Starting fresh processing\")\n","\n","    # Find which rows are not processed yet\n","    # Assuming if 'words' column is NaN, the row hasn't been processed\n","    # unprocessed_mask = df['words'].isna()\n","    # unprocessed_indices = df[unprocessed_mask].index\n","    unprocessed_indices = find_unprocessed_indices(df, 'words')\n","\n","    if len(unprocessed_indices) == 0:\n","        print(\"All rows already processed\")\n","        df.to_csv(output_path, index=False, sep=sep)\n","        return df\n","\n","    total_chunks = (df.shape[0] + chunk_size - 1) // chunk_size\n","\n","    current_chunks = (len(unprocessed_indices) + chunk_size - 1) // chunk_size\n","\n","    for current_chunk_num in range(current_chunks):\n","        try:\n","            chunk_start = current_chunk_num * chunk_size\n","            chunk_end = min((current_chunk_num + 1) * chunk_size, len(unprocessed_indices))\n","            current_indices = unprocessed_indices[chunk_start:chunk_end]\n","            start_idx = current_indices[0]\n","            end_idx = current_indices[-1]\n","\n","            print(f\"Processing chunk {current_chunk_num + 1}/{current_chunks} (indices [{start_idx}-{end_idx}]/{df.shape[0]})\")\n","\n","            # Process current chunk in a vectorized way\n","            chunk_sentences = df.loc[current_indices, 'sentence']\n","            results = chunk_sentences.apply(process_func).apply(pd.Series)\n","\n","            # Update the dataframe with processed results\n","            results.columns = [\"words\", \"lemmas\", \"pos\", \"morph\", \"features\", \"dep\", \"n_words\"]\n","            df.loc[current_indices, results.columns] = results\n","\n","            # Save checkpoint after each chunk\n","            latest_filename = save_checkpoint(output_path, df, sep=sep)\n","            print(f\"Saved checkpoint {latest_filename}\")\n","\n","        except KeyboardInterrupt:\n","            print(\"\\nProcessing interrupted. Progress saved in checkpoint file.\")\n","            save_checkpoint(output_path, df, sep=sep)\n","            return df\n","        except Exception as e:\n","            print(f\"Error processing chunk: {e}\")\n","            save_checkpoint(output_path, df, sep=sep)\n","            raise\n","\n","    # All processing complete, save final output\n","    unprocessed_indices = find_unprocessed_indices(df, 'words')\n","    if len(unprocessed_indices) == 0:\n","        print(\"Processing complete\")\n","        df.to_csv(output_path, index=False, sep=sep)\n","\n","    return df"]},{"cell_type":"markdown","metadata":{"id":"SNvo-BqxanNS"},"source":["#### Split into two parts to be processed in parallel"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":280,"status":"ok","timestamp":1737993592932,"user":{"displayName":"Miroslava Ivanova","userId":"15457167696903657486"},"user_tz":-120},"id":"MetKyD5NbcJ-","outputId":"340885c8-d930-4557-f5f3-05709607b658"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(6690845, 1)"]},"metadata":{},"execution_count":7}],"source":["df.shape"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":268,"status":"ok","timestamp":1737993595400,"user":{"displayName":"Miroslava Ivanova","userId":"15457167696903657486"},"user_tz":-120},"id":"_qTAPH76bfHd","outputId":"c51bae51-90a9-4747-e647-f9932cc56577"},"outputs":[{"output_type":"stream","name":"stdout","text":["df1 shape: (3345422, 1)\n","df2 shape: (3345423, 1)\n"]}],"source":["# split the df into two dataframes for parallel processing\n","df1 = df.iloc[:len(df)//2]\n","df2 = df.iloc[len(df)//2:]\n","\n","print(f\"df1 shape: {df1.shape}\")\n","print(f\"df2 shape: {df2.shape}\")"]},{"cell_type":"markdown","source":["##### Part 1: Add pos and morph tagging using UDPipe"],"metadata":{"id":"jdCIbxvPwV-8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RzGxWKn5dT5I"},"outputs":[],"source":["output_path = f'{data_processed_dir}/sent_fiction_nlp_features_part1_v1.tsv'\n","\n","if not LOAD_SAVED_DATA:\n","    chunk_size = 10000\n","    df_pos_udpipe_part1 = process_dataframe_with_checkpoints(\n","        df1,\n","        convert_udpipe_to_spacy,\n","        chunk_size=chunk_size,\n","        output_path=output_path,\n","        sep = '\\t'\n","    )"]},{"cell_type":"markdown","metadata":{"id":"ZGZtBNf8zvpu"},"source":["##### Part 1: Add gender, number, person"]},{"cell_type":"markdown","source":["> explain that this extracts gender, number and person"],"metadata":{"id":"x5Oct6y7LF5d"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"NFZHj3Cb38kn"},"outputs":[],"source":["if not LOAD_SAVED_DATA:\n","    input_path = f'{data_processed_dir}/sent_fiction_nlp_features_part1_v1.tsv'\n","    output_path = f'{data_processed_dir}/sent_fiction_nlp_features_part1_v2.tsv'\n","    chunk_size = 100_000\n","\n","    # Initialize CSV reader with chunks\n","    reader = pd.read_csv(input_path, chunksize=chunk_size, low_memory=True, sep=\"\\t\")\n","\n","    # Process and write chunks incrementally\n","    for i, chunk in enumerate(reader):\n","        processed_chunk = process_chunk(chunk)\n","        # Write header only for the first chunk\n","        mode = \"w\" if i == 0 else \"a\"\n","        header = i == 0\n","        processed_chunk.to_csv(output_path, mode=mode, header=header, index=False, sep='\\t')\n","        print(f\"Processed chunk {i+1} with {len(processed_chunk)} rows\")"]},{"cell_type":"markdown","metadata":{"id":"YJUrkJG1bd-F"},"source":["##### Part 1: Add case"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CVHQjgaUbk7G"},"outputs":[],"source":["input_path = f'{data_processed_dir}/sent_fiction_nlp_features_part1_v2.tsv'\n","\n","if not LOAD_SAVED_DATA:\n","    df_pos_udpipe_part1 = pd.read_csv(input_path, low_memory=True, sep=\"\\t\")\n","    df_pos_udpipe_part1[\"morph\"] = df_pos_udpipe_part1[\"morph\"].apply(ast.literal_eval)\n","    results = df_pos_udpipe_part1[\"morph\"].apply(parse_nlp_morphtags).apply(pd.Series)\n","    results.columns = [\"case\"]\n","    df_pos_udpipe_part1[results.columns] = results"]},{"cell_type":"markdown","metadata":{"id":"tBbhadI9haa4"},"source":["##### Part 1: Convert to lowercase"]},{"cell_type":"code","source":["if not LOAD_SAVED_DATA:\n","    columns_to_lower = ['sentence', 'words', 'lemmas']\n","    df_pos_udpipe_part1[columns_to_lower] = df_pos_udpipe_part1[columns_to_lower].apply(lambda x: x.str.lower())"],"metadata":{"id":"tB_gd7HYpZzv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"asUc4u5R_svG"},"source":["##### Part 1: Remove columns with repeated information and save the final version"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"euPim0ilfCxD"},"outputs":[],"source":["output_path = f'{data_processed_dir}/sent_fiction_nlp_features_part1_v3_final.csv'\n","\n","if not LOAD_SAVED_DATA:\n","    df_pos_udpipe_part1_no_repeat = df_pos_udpipe_part1.drop(columns=['morph', 'features', 'dep'])\n","\n","    df_pos_udpipe_part1_no_repeat.to_csv(output_path, index=False, sep='\\t')"]},{"cell_type":"markdown","source":["###### Part 2: Add pos and morph tagging using UDPipe"],"metadata":{"id":"zoPgXZRXwp65"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yjj5A7--wp65"},"outputs":[],"source":["output_path = f'{data_processed_dir}/sent_fiction_nlp_features_part2_v1.tsv'\n","\n","if not LOAD_SAVED_DATA:\n","    chunk_size = 10000\n","    df_pos_udpipe_part2 = process_dataframe_with_checkpoints(\n","        df2,\n","        convert_udpipe_to_spacy,\n","        chunk_size=chunk_size,\n","        output_path=output_path,\n","        sep = '\\t'\n","    )"]},{"cell_type":"markdown","metadata":{"id":"zCYhBSfqwp65"},"source":["###### Part 2: Add gender, number, person"]},{"cell_type":"markdown","source":["> explain that this extracts gender, number and person\n","> ðŸ’™mii rename process_chunk to process_features\n"],"metadata":{"id":"CVzg9PXOwp66"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DLedAyH7wp66"},"outputs":[],"source":["if not LOAD_SAVED_DATA:\n","    input_path = f'{data_processed_dir}/sent_fiction_nlp_features_part2_v1.tsv'\n","    output_path = f'{data_processed_dir}/sent_fiction_nlp_features_part2_v2.tsv'\n","    chunk_size = 100_000\n","\n","    # Initialize CSV reader with chunks\n","    reader = pd.read_csv(input_path, chunksize=chunk_size, low_memory=True, sep=\"\\t\")\n","\n","    # Process and write chunks incrementally\n","    for i, chunk in enumerate(reader):\n","        processed_chunk = process_chunk(chunk)\n","        # Write header only for the first chunk\n","        mode = \"w\" if i == 0 else \"a\"\n","        header = i == 0\n","        processed_chunk.to_csv(output_path, mode=mode, header=header, index=False, sep='\\t')\n","        print(f\"Processed chunk {i+1} with {len(processed_chunk)} rows\")"]},{"cell_type":"markdown","metadata":{"id":"q2FURkOhwp66"},"source":["###### Part 2: Add case"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"UH4ejR5jwp66","executionInfo":{"status":"ok","timestamp":1737994280765,"user_tz":-120,"elapsed":271,"user":{"displayName":"Miroslava Ivanova","userId":"15457167696903657486"}}},"outputs":[],"source":["input_path = f'{data_processed_dir}/sent_fiction_nlp_features_part2_v2.tsv'\n","\n","if not LOAD_SAVED_DATA:\n","    df_pos_udpipe_part2 = pd.read_csv(input_path, low_memory=True, sep=\"\\t\")\n","    df_pos_udpipe_part2[\"morph\"] = df_pos_udpipe_part2[\"morph\"].apply(ast.literal_eval)\n","    results = df_pos_udpipe_part2[\"morph\"].apply(parse_nlp_morphtags).apply(pd.Series)\n","    results.columns = [\"case\"]\n","    df_pos_udpipe_part2[results.columns] = results"]},{"cell_type":"code","source":["df_pos_udpipe_part2 = pd.read_csv(input_path, low_memory=True, sep=\"\\t\")"],"metadata":{"id":"7wcaLhTWiCmo","executionInfo":{"status":"ok","timestamp":1737994418974,"user_tz":-120,"elapsed":131149,"user":{"displayName":"Miroslava Ivanova","userId":"15457167696903657486"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v_IMney4wp66"},"source":["###### Part 2: Convert to lowercase"]},{"cell_type":"code","source":["if not LOAD_SAVED_DATA:\n","    columns_to_lower = ['sentence', 'words', 'lemmas']\n","    df_pos_udpipe_part2[columns_to_lower] = df_pos_udpipe_part2[columns_to_lower].apply(lambda x: x.str.lower())"],"metadata":{"id":"xDsmZy_rwp66","executionInfo":{"status":"ok","timestamp":1737994471433,"user_tz":-120,"elapsed":16066,"user":{"displayName":"Miroslava Ivanova","userId":"15457167696903657486"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rc6zr-tfwp66"},"source":["###### Part 2: Remove columns with repeated information and save the final version"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"utYLr0VRwp67","executionInfo":{"status":"ok","timestamp":1737994565695,"user_tz":-120,"elapsed":83014,"user":{"displayName":"Miroslava Ivanova","userId":"15457167696903657486"}}},"outputs":[],"source":["output_path = f'{data_processed_dir}/sent_fiction_nlp_features_part2_v3_final.csv'\n","\n","if not LOAD_SAVED_DATA:\n","    df_pos_udpipe_part2_no_repeat = df_pos_udpipe_part2.drop(columns=['morph', 'features', 'dep'])\n","\n","    df_pos_udpipe_part2_no_repeat.to_csv(output_path, index=False, sep='\\t')"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true,"collapsed_sections":["wTOlydwLD0mL","2wydS4HBANtb","fEhXJKXsmL_m","jdCIbxvPwV-8","ZGZtBNf8zvpu"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}